#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# 
# # Практическое задание 5
# 
# ## Общая информация
# 
# Дата выдачи: 26.02.2017
# 
# Срок сдачи: 23:59MSK 12.03.2017
# 
# ### О задании
# 
# Практическое задание 5 посвящено использованию ядер в методах машинного обучения. В рамках данного задания вы:
#  * исследуете вид разделяющих поверхностей в исходном пространстве в зависимости от типа используемов ядра и значений его параметров;
#  * используете ядра для решения реальной задачи;
#  * реализуете пользовательское ядро для использования в ядровом SVM;
#  * используете аппроксимации ядер;
#  * поймёте, почему ядра — это не всегда хорошо.
# 
# ### Оценивание и штрафы
# 
# Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.
# 
# Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.
# 
# Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце Вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник). 
# 
# Неэффективная реализация кода может негативно отразиться на оценке.
# 
# 
# ### Формат сдачи
# Для сдачи задания переименуйте получившийся файл \*.ipynb в соответствии со следующим форматом: *HW5_Username.ipynb*, где *Username* — Ваша фамилия и инициалы на латинице (например, *HW5_IvanovII.ipynb*). Далее отправьте этот файл на hse.cs.ml+<номер группы>@gmail.com (например, hse.cs.ml+141@gmail.com для студентов группы БПМИ-141).

# ## Ядровой SVM
# 
# **1. (0 баллов)** Сгенерируйте три случайные двумерные выборки для бинарной классификации (хотя бы по 400 точек в каждой):
# - с линейно разделимыми классами;
# - с хорошо разделимыми классами, но не линейно разделимыми;
# - с плохо разделимыми классами.
# 
# Визуализируйте полученные выборки на плоскости.
# 
# Для генерации случайной выборки можно использовать функции из модуля [sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets).

# In[ ]:


# Your code here

# В [лекции 13](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture13-features.pdf) была сформулирована двойственная задача метода опорных векторов:
# $$\begin{cases}
# \sum_{i=1}^l \lambda_i - \frac{1}{2} \sum_{i, \, j =1}^l \lambda_i \lambda_j y_i y_j \langle x_i, x_j\rangle \to \max_\lambda,\\
# 0 \le \lambda_i \le C, i = \overline{1, l},\\
# \sum_{i=1}^l \lambda_i y_i = 0.
# \end{cases}$$
# 
# После решения данной задачи прогнозы для новых объектов строятся следующим образом: $a(x) = \text{sign} \left( \sum_{i=1}^l \lambda_i y_i \langle x_i, x \rangle + b \right).$
# 
# Заметим, что и оптимизационная задача, и формула итогового классификатора зависят лишь от скалярных произведений объектов, а потому в данном методе можно использовать ядра для восстановления нелинейных зависимостей.

# **2. (1.5 балла)** Обучите на сгенерированных ранее двумерных выборках [ядровой SVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) с использованием следующих типов ядер (для различных значений гиперпараметра $C$):
# - линейное: $K(x, z) = \langle x, z \rangle$;
# - полиномиальное: $K(x, z) = (\gamma \langle x, z \rangle + 1)^d$ (для различных значений $\gamma, d$);
# - гауссовское: $K(x, z) = \exp(-\gamma \|x - z\|^2)$ (для различных значений $\gamma$).
# 
# Визуализируйте разделяющую поверхность и разделяющую полосу:
#  - для модели с линейным ядром для различных значений $C$;
#  - для моделей, использующих полиномиальное и гауссовское ядро, соответствующих недообучению, нормальному поведению и переобучению.

# In[ ]:


# Your code here

# **3. (0.5 балла)** Ответьте на следующие вопросы:
#  - Как ведет себя SVM с полиномиальным ядром в зависимости от значений гиперпараметра $C$, степени ядра $d$ и параметра $\gamma$?
#  - Как ведет себя SVM с гауссовским ядром в зависимости от значений гиперпараметра $C$ и $\gamma$?

# **Ответ:**

# **4. (0.5 балла)** Обучите модели с использованием ядер из п. 2 для задачи бинарной классификации [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse) (используйте файл train.csv) для значения $C=1.$ Для оценки качества разбейте выборку на обучающую и тестовую в отношении 50/50. Постройте модель, позволяющую достичь значения accuracy, равного 0.75, на тестовой выборке. Позволяет ли использование ядер достичь лучшего качества по сравнению с линейной моделью?

# In[ ]:


# Your code here

# **Ответ:**

# ## Аппроксимации ядер
# 
# К сожалению, использование ядер, особенно при больших размерах выборок, как правило, значительно увеличивает сложность процедуры обучения модели. Для решения этой проблемы используется аппроксимация спрямляющего пространства — в частности, в рамках курса рассматривались [метод Нистрома](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem) и [метод случайных признаков Фурье](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler).

# **5. (2 балла)** Модифицируйте модель с использованием ядра из п. 4 путём аппроксимаций ядер, реализованных в модуле [sklearn.kernel_approximation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.kernel_approximation). Постройте график зависимости accuracy на тестовой выборке в зависимости от значения параметра n_components, используемого при аппроксимации. Какие выводы можно сделать?

# In[ ]:


# Your code here

# ## All-subsequences kernel

# Попробуем при помощи ядрового SVM с использованием all-subsequences-kernel (описано в [материалах семинара 14](https://github.com/esokolov/ml-course-hse/blob/master/2016-spring/seminars/sem14-kernels.pdf)) решить задачу [20newsgroups](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) классификации текстов статей по темам.
# 
# **6. (0 баллов)** Загрузите двухклассовую обучающую и тестовую выборки (используя параметр subset функции [fetch_20newsgroups](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)) для категорий rec.autos и soc.religion.christian со следующими значениями параметров:
#     * shuffle=True;
#     * random_state=1;
#     * remove=('headers', 'footers', 'quotes').
# Загрузите выборки таким образом, чтобы целевая переменная принимала значения $\pm1$.

# In[ ]:


# Your code here

# К сожалению, для корректной работы sklearn.SVC с использованием пользовательского ядра необходимо, чтобы объекты являлись числовыми векторами, что не выполняется для необходимого ядра.
# 
# **7. (0.5 балла)** Преобразуйте матрицы объект-признак обучающей и контрольной выборок следующим образом:
#  * удалите все символы, кроме букв и пробельных символов;
#  * приведите все буквы к нижнему регистру;
#  * разбейте текст на токены по пробельным символам;
#  * удалите токены длиной $\le 3$ символов;
#  * закодируйте каждый объект выборки последовательностью **номеров** токенов в некотором "словаре", состоящем из всех токенов, встречающихся в обучающей выборке; для каждого текста кодируйте 30 первых токенов.
#  
# Способ кодирования токенов тестовой выборки, не встречающихся в обучающей, а также способ кодирования объектов, состоящих из менее чем 30 токенов, остаётся на ваше усмотрение, однако опишите, как именно были решены эти проблемы.

# In[ ]:


# Your code here

# **8. (1 балл)** Реализуйте функцию, принимающую в качестве параметров 2 строки x и z, и возвращающую значение all-subsequences-kernel для этих строк.

# In[ ]:


def all_subseq_kernel(x, z):
    # Your code here

# **9. (0.5 балла)** Реализуйте функцию, которая принимает в качестве параметров 2 выборки $\{x_i\}_{i=1}^l$ и $\{z_j\}_{j=1}^n$, закодированные описанным ранее способом, а также используемый словарь токенов, и возвращает матрицу $K = (K_{ij})_{i, j=1}^{l, n},$ где $K_{ij} = \text{all_subseq_kernel}(x_i, z_j).$
# 
# Для выполнения данного задания можно использовать функцию [sklearn.metrics.pairwise.pairwise_kernels](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html).

# In[ ]:


def all_subseq_kernel_matrix(X, Z, token_list):
    # Your code here

# **10. (0.5 балла)** Обучите SVM с параметрами по умолчанию с использованием all-subsequences-kernel на обучающей выборке и выведите значение accuracy на тестовой. В случае вычислительных проблем используйте матрицы Грама для обучения и теста, сохраненные в файлах homework-practice-05-train-kernel-matrix.csv и homework-practice-05-test-kernel-matrix.csv соответственно. Обратите внимание, что значения ядра для полученных текстов довольно велики в силу их длины, что может повлечь численные проблемы в процессе обучения модели. Поэтому рекомендуется вместо исходного рассматривать нормированное ядро $\tilde{K}(x, z) = \langle \frac{\phi(x)}{\| \phi(x) \|}, \frac{\phi(z)}{\| \phi(z) \|} \rangle.$ Перейти к нему можно с использованием значений диагональных элементов соответствующих матриц Грама.

# In[ ]:


# Your code here

# Иногда в случае больших размеров обучающих выборок используют метод, схожий с аппроксимациями ядер. После решения двойственной задачи ядрового SVM прогноз для нового объекта вычисляется следующим образом:
# $$a(x) = sign \left( \sum_{j=1}^l \lambda_j y_j K(x_j, x) + b \right).$$
# 
# Такое представление позволяет поставить и решить несколько другую задачу. Пусть параметрами модели являются переменные $\lambda_1, \dots, \lambda_l, b$ — в этом случае можно построить линейный классификатор на обучающей выборке в новом признаковом пространстве $Z = (z_{ij})_{i, j=1}^{l, l},$ где $z_{ij} = y_j K(x_i, x_j).$ При этом количество признаков в новом признаковом пространстве будет равно $l$, и каждый из них будет соответствовать некоторому объекту обучающей выборки, поэтому можно взять лишь произвольное подмножество из них.
# 
# **11. (0.5 балла)** Сформируйте обучающую и тестовую выборки в новом признаковом пространстве, как описано выше, обучите логистическую регрессию с параметрами по умолчанию и выведите значение accuracy на тестовой выборке. Постройте графики зависимости accuracy на тестовой выборке в зависимости от количества признаков в новом признаковом пространстве.

# In[ ]:


# Your code here

# **12. (1 балл)** Чем, на ваш взгляд, обусловлено такое низкое качество классификации для рассмотренных моделей при использовании all-subsequences-kernel? Какие шаги можно предпринять для улучшения качества?
# 
# **Ответ**:

# **13. (1.5 балла)** Улучшите какую-либо модель из пп. 10-11 путём модификации выборки или подбора параметров модели таким образом, чтобы accuracy на той же обучающей выборке, что использовалась ранее, достигало значения 0.58. Опишите ваши изменения модели.

# In[ ]:


# Your code here

# **Ответ:**

# **14. (3 доп. балла)** Реализуйте ядро, которое позволяет достичь значения accuracy на тестовой выборке, равного 0.7, при классификации при помощи ядрового SVM.

# In[ ]:


# Your code here

# Здесь вы можете поделиться своими мыслями по поводу этого задания.

# In[ ]:




# А здесь — вставить вашу любимую картинку.

# In[ ]:



