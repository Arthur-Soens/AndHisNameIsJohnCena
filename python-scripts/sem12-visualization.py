#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# # Семинар 12. Визуализация данных

# Виузализация данных - первый шаг в решении практически любой задачи анализа данных, в частности, при участии в соревновании. Визуализация помогает ответить на следующие вопросы:
# * какую предобработку данных нужно провести
# * какие признаки/объекты могут быть полезными/вредными для решения
# * какие методы лучше использовать для предсказания
# * какие признаки добавить
# * каковы особенности задачи.
# 
# Семинар состоит из двух частей: в первой мы рассмотрим основные приемы визуализации данных на примере задачи с kaggle, а во второй познакомимся с некоторыми примерами удачного применения визуализации для решения задачи.
# 
# ## Визуализация данных Prudential
# 
# Мы будем работать c данными [kaggle-соревнования Prudential Life Insurance Assessment](https://www.kaggle.com/c/prudential-life-insurance-assessment), завершившегося зимой 2016 года. 
# 
# __Задача соревнования:__ по характеристикам человека и его заявки на медицинскую страховку предсказать степень риска, характериную для данной страховки.
# 
# Целевой признак (степень риска) в задаче номинальный: целые числа от 1 до 8, а метрика качества, предложенная организаторами соревнования, оценивает степень согласованности двух рейтингов. Поэтому задачу можно решать и методами классификации, и методами регрессии (в последнем случае придется округлять предсказания). Это стоит учитывать при анализе результатов визуализации.
# 
# __Метрика качества:__ [quadratic weighted kappa](https://www.kaggle.com/c/prudential-life-insurance-assessment/details/evaluation). Метрика измеряет качество согласованности двух рейтингов и изменяется от 0 (случайная расстановка меток) до 1 (полная согласованность).
# 
# __Чем будем заниматься мы:__
# * рассматривать данные;
# * пробовать разные методы визуализации;
# * анализировать графики и пытаться сделать выводы, полезные для следующих этапов решения задачи.
# 
# Для визуализации мы будем использовать модуль seaborn.

# In[1]:


import numpy as np
import pandas
from matplotlib import pyplot as plt
import seaborn

# Считываем данные:

# In[3]:


data = pandas.read_csv("Prudential/train.csv")
test = pandas.read_csv("Prudential/test.csv")

# In[4]:


data.head()

# In[5]:


test.head()

# ### Анализ Id  - можно ли его удалить?

# Среди признаков присутствует Id. Смысловой нагрузки он, как правило, не несет, если только он не предназначен для связывания нескольких таблиц. Однако по Id иногда можно узнать принцип упорядоченности данных. Если среди признаков есть Id и время, можно строить scatter этой пары. У нас признака времени нет, попробуем строить scatter пары Id - Response и Id - другой признак.

# In[8]:


# scatter Id - Respose для обучающей выборки
plt.figure(figsize=(20, 2))
_ = plt.scatter(data["Id"], data["Response"], alpha=0.1, color="blue")

# По графику видно только то, что категории 3 и 4 реже других встречаются в Response. Обратите внимание, что при построении scatter'ов рекомендуется делать точки полупрозрачными (alpha < 1). Это позволяет обнаружить области с меньшей плотностью, а также уменьшает влияние основного недостатка scatter'ов - зависимости от порядка отрисовки точек. Например, если мы сначала рисуем синие точки, а потом красные, и все точки плотные, то красные могут просто закрыть синие. Постоим scatter Id - Insurance_History_2:

# In[9]:


# scatter Id - Insurance_History_2 для обучения (синие точки) и контроля (красные точки)
plt.figure(figsize=(20, 2))
feature = "Insurance_History_2"
plt.scatter(data["Id"], data[feature], alpha=0.1, color="blue")
_ = plt.scatter(test["Id"], test[feature], alpha=0.1, color="red")

# По этому графику можно сделать ошибочный вывод, что в контроле все значения Insurance_History_2 равны 0. Поменяем порядок отрисовки:

# In[10]:


# scatter Id - Insurance_History_2 для обучения (синие точки) и контроля (красные точки)
plt.figure(figsize=(20, 2))
feature = "Insurance_History_2"
plt.scatter(test["Id"], test[feature], alpha=0.1, color="red")
_ = plt.scatter(data["Id"], data[feature], alpha=0.1, color="blue")

# Итак, здесь снова можно сделать вывод только о разном размере категорий (в категорию 1 объекты попадают реже). 
# 
# Остается посмотреть на распределение номеров между обучением и контролем:

# In[11]:


# scatter Id - 0 (объект из контроля) и 1 (объект из обучения)
plt.figure(figsize=(20, 2))
plt.scatter(test["Id"], [0]*test.shape[0], alpha=0.1)
_ = plt.scatter(data["Id"], [1]*data.shape[0], alpha=0.1)

# Видно, что в контроле данных больше, а номера распределены случайно равномерно. __Итог: можем удалить Id из признаков.__

# ### Типы признаков

# Часто в соревнованиях смысл признаков не известен, в нашем соревновании это не совсем так. Согласно описанию признаков на [странице с данными](https://www.kaggle.com/c/prudential-life-insurance-assessment/data), значение известно только для нескольких признаков, для остальных известна только группа, к которой этот признак принадлежит (например, медицинские данные) и тип признака: вещественный, целочисленный или категориальный.
# 
# Создадим три списка признаков, соответствующие их группам: вещественные, целочисленные и категориальные (эти списки даны на странице соревнования).
# 
# Если бы типы признаков были не даны, нам бы пришлось вручную просматривать все 128 признаков, чтобы понять, какие значения они принимают. 

# In[12]:


real_features = ["Product_Info_4", "Ins_Age", "Ht", "Wt", "BMI", "Employment_Info_1", "Employment_Info_4", "Employment_Info_6",
                 "Insurance_History_5", "Family_Hist_2", "Family_Hist_3", "Family_Hist_4", "Family_Hist_5"]
discrete_features = ["Medical_History_1", "Medical_History_10", "Medical_History_15", "Medical_History_24", "Medical_History_32"]
cat_features = data.columns.drop(real_features).drop(discrete_features).drop(["Id", "Response"]).tolist()

# ### Первичный анализ

# Есть набор стандартных приемов визуализации, которые нужно попробовать, когда вы начинаете работу с набором данных; к ним относятся:
# * построение гистограмм признаков (histogram, density estimation), 
# * вычисение статистик, 
# * оценка зависимости целевого признака от остальных (boxplot, scatterplot, violinplot), 
# * визуализация пар признаков (как правило, scatterplot). 
# 
# Конкретный вид графика, который вам подходит, зависит от типа признаков, хороший обзор приведен в разделе Plotting functions [туториала seaborn](https://stanford.edu/~mwaskom/software/seaborn/tutorial.html).
# 
# Сначала рассмотрим чиловые признаки, затем - категориальные.
# 
# ##### Выведем статистики вещественных и целочисленных признаков:

# In[13]:


data[real_features].describe()

# Видим, что признаки имеют одинаковый масштаб. Есть признаки с пропущенными значениями, в том числе с большим количеством пропусков (например, Family_Hist_5).

# In[14]:


data[discrete_features].describe()

# Целочисленные признаки так же приведены к одному масштабу, но они в большинстве своем известны для малого числа объектов. Возможно, такие признаки лучше исключить из рассмотрения.

# ##### Построим гистограммы вещественных и целочисленных признаков.
# Вместо того, чтобы в цикле по признакам строить отдельно каждую гистограмму, стоит воспользоваться методом hist датафрейма. Выведем отдельно гистограммы вещественных и целочисленных признаков.

# In[15]:


_ = data[real_features].hist(figsize=(20, 20), bins=100)

# In[16]:


_ = data[discrete_features].hist(figsize=(10, 10), bins=100)

# Среди числовых признаков нет константных, но есть такие, у которых доминирует одно значение (например, Insurance_History_5, Employment_Info_5, Medical_History_15, Medical_History_32). Для таких признаков имеет смысл создавать дополнительные бинарные переменные, показывающие, равно ли значение признака доминирующему (правда, если верить форуму, в этом соревновании feature engineering особенно не помогал). То же можно делать при наличии в распределении некоторых "точек перелома", как у признака "Ins_Age" ("разлом" в районе 0.3).
# 
# Кроме того, по гистограммам можно находить шумовые (нетипичные) объекты. Например, на гистограмме BMI видно всплеск около 1.
# 
# Если предполагается использование вероятностных методов, по гистограммам можно оценивать распределение данных. В данном случае видно, что данные не стоит описывать многомерным нормальным распределением, потому что есть признаки, распределение которых скошено или имеет тяжелые хвосты.
# 
# Аналог гистограммы - непараметрическое восстановление плотности. Применим этот метод визуализации распределний признаков для сравнения распределений в обучении и контроле:

# In[17]:


fig, axes = plt.subplots(3, 6, figsize=(18, 9))
i = 0
for feature in real_features+discrete_features:
    seaborn.kdeplot(data[feature], ax=axes[i / 6, i % 6], label="Train")
    seaborn.kdeplot(test[feature], ax=axes[i / 6, i % 6], label="Test")
    i += 1
fig.tight_layout()

# Видим, что распределение данных в обучениии контроле практически одинаковое. Если бы это было не так, модель, обученная на первой части выборки, вероятно плохо работала бы на новых (тестовых) данных.

# ##### Теперь визуализируем признаки попарно. 
# Построим scatterplot для пар вещественных признаков. Для этого в seaborn есть функция pairplot. Исключим признаки, распределение которых не похоже на колокол (хотя бы скошенный и неровный), тем самым мы уменьшим размер таблицы пар. На диагоналях таблицы будут стоять оценки распределений признаков (гистограммы или восстановленные плотности, diag_kind="hist" или "kde"). Если указать параметр hue = дискретный целевой признак, то разные его значения будут отображаться разными цветами.

# In[18]:


_ = seaborn.pairplot(data[real_features+["Response"]].drop(
        ["Employment_Info_4", "Employment_Info_6", "Insurance_History_5", "Product_Info_4"], axis=1), 
        hue="Response", diag_kind="kde")

# Классы накрывают друг друга, и графики не очень информативны. Тем не менее, по диагональным и боковым графикам можно увидеть, что многие признаки меняют распределение для разных значений целевого признака, т. е. эти признаки должны быть важными при построении модели. Можно сказать, что признаки не являются независимыми.
# 
# Кроме того, вновь можно заметить шумовые (нестандартные) объекты, которые, возможно, стоит удалить во время обучения. 

# Теперь построим такие же графики для целочисленных признаков (никакие признаки удалять не нужно, потому что таких признаков и так немного).

# In[19]:


_ = seaborn.pairplot(data[discrete_features+["Response"]], hue="Response", diag_kind="kde")

# Графики выглядят еще менее информативно. Заметна тенденция, что пары признаков сконцентрированы либо на сторона квадрата [0, 240] x [0, 240], либо на его диагонали, то есть признаки как-то связаны.

# ##### Посмотрим на корреляции признаков, чтобы узнать, не нужно ли удалять какие-то признаки перед (гипотетическим) построением модели. 
# 
# Для визуализации матрицы попарных корреляций удобно использовать функцию seaborn.heatmap, она автоматически подпишет признаки на осях и покажет colorbar. Мы вычислим корреляции только между вещественными признками.

# In[20]:


_ = seaborn.heatmap(data[real_features].corr(), square=True)

# У нас есть пара сильно коррелирующих признаков: Family_Hist_2 и Family_Hist_4.

# Перейдем к визуализации категориальных признаков.
# 
# ##### Посчитаем количество значений для каждого признака.
# Для этого создаем новую pyplot-фигуру, указываем, сколько графиков на ней будет, задаем размер; параметр sharey говорит не подписывать все оси отдельно, а подписать их только один раз слева; здесь это уместно, потому что масштабы всех счетчиков одни и те же. 
# 
# В цикле по всем категориальным признакам строим countplot признака с помощью seaborn. Указываем параметр data, какой признак виузализировать, а также передаем ссылку на конкретную ячейку таблицы, в которой нужно изобразить график. Признаков всего 108, поэтому последние две ячейки таблицы графиков размером 11 x 10 останутся пустыми.

# In[21]:


fig, axes = plt.subplots(11, 10, figsize=(20, 20), sharey=True)
for i in range(len(cat_features)):
    seaborn.countplot(x=cat_features[i], data=data, ax=axes[i / 10, i % 10])
fig.tight_layout()

# Среди признаков нет константных, но есть бинарные с редкими категориями. Учитывая, что среди категориальных признаков много медицинских показателей, можно предположить, что именно эти признаки сильно влияют на увеличение риска (целевой признак). Проверить это предположение можно, построив такие же countplot с разбивкой каждого значения дополнительно по классам (то есть у нас будет несколько групп столбиков, и в каждой группе 8). Это можно сделать, указав параметр hue в этой функции аналогично тому, как мы это делали выше.
# 
# Построим графики countplot для признаков 'Medical_Keyword_23', 'Medical_Keyword_39', 'Medical_Keyword_45' (признаки выбраны случайно) с разбивкой по классам.

# In[22]:


fig, axes = plt.subplots(1, 3, figsize=(20, 8))
seaborn.countplot(x='Medical_Keyword_23', data=data, hue="Response", ax=axes[0])   # 98, 104
seaborn.countplot(x='Medical_Keyword_39', data=data, hue="Response", ax=axes[1])
_ = seaborn.countplot(x='Medical_Keyword_45', data=data, hue="Response", ax=axes[2])

# Признак Medical_Keyword_23 меняет распределение при Response=1, а два других не меняют.

# ##### Посмотрим на распределение целевого признака, чтобы узнать, сбалансированы ли классы:

# In[23]:


_ = seaborn.countplot(data.Response)

# В категорию 8 люди попадают чаще, чем в другие категории.

# ##### Визуализация пар признак - целевой признак
# Такая визуализация полезна, чтобы оценить важность отдельных признаков (конечно, приблизительно, потому что иногда признаки влияют на целевую переменную в комбинации с несколькими другими), а также применимость разных моделей (например, линейных).
# 
# Для вещественных признаков можно строить:
# * scatter-plot с полупрозрачными точками - удобно, если целевой признак вещественный
# * [boxplot](http://seaborn.pydata.org/generated/seaborn.boxplot.html) - удобно, если целевой признак бинарный / категориальный.

# In[24]:


_ = plt.scatter(data["Family_Hist_3"], data["Response"], alpha=0.1)

# В данной задаче по scatter-графикам для вещественных признаков увидеть что-либо интересное сложно. Посмотрим на boxplot:

# In[26]:


fig, axes = plt.subplots(3, 6, figsize=(18, 9))
i = 1
for feature in real_features+discrete_features:
    plt.subplot(3, 6, i)
    seaborn.boxplot(y=feature, x="Response", data=data)
    i += 1
fig.tight_layout()

# Квантильные характеристики распределения признака отличаются в зависимости от BMI, Employment_Info_6, группы признаков Medical_History.

# ### Визуализация с помощью понижения размерности

# Далее можно воспользоваться средствами понижения размерности. Для задачи с дискретным целевым признаком это позволит понять, какие классы хорошо разделяются, а какие - нет.
# 
# Такие методы строят матрицу попарных расстояний между объектами, которая в случае, когда объектов много, будет занимать много памяти. Кроме того, отображать много точек на scatter plot (а именно его используют для визуализации результата понижения размерности) неудобно. Поэтому мы перемешаем выборку (и далее будем использовать ее) и выберем првые 1000 объектов для понижения размерности.

# In[27]:


from sklearn.utils import shuffle
from sklearn.preprocessing import scale

# In[28]:


sdata = shuffle(data, random_state=321)

# Методы sklearn не принимают матрицы с пропусками (nan). Чтобы избежать этой проблемы, не будем рассматривать признаки, которые имеют много пропусков (последние четыре в списке вещественных признаков). Кроме того, ограничимся расмотрением вещественных признаков.
# 
# В следующей ячейке мы отбираем нужные признаки, затем находим объекты, у которых все признаки известны (нет пропусков в выбанных признаках), а затем создаем отдельно матрицу объекты-признаки для работы методов понижения размерности и отдельно вектор правильных ответов на этих объектах: data_subset и response_subset. Кроме того, мы дополнительно стандартизуем нашу маленькую выборку, потому что методы понижения размерности очень чувствительны к разномасштабным данным (это отдельно указано в [туториале](http://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling) в sklearn)

# In[29]:


subset_l  = 1000
selected_features = real_features[:-4]
objects_with_nan = sdata.index[np.any(np.isnan(sdata[selected_features].values), axis=1)]   
data_subset = scale(sdata[selected_features].drop(objects_with_nan, axis=0)[:subset_l])
response_subset = sdata["Response"].drop(objects_with_nan, axis=0)[:subset_l]

# Будем строить визуализацию с помощью наибоее популярных методов нелинейного понижения размерности: t-SNE и MDS.

# In[30]:


from sklearn.manifold import TSNE
import matplotlib.cm as cm # импортируем цветовые схемы, чтобы рисовать графики.

# In[31]:


tsne = TSNE(random_state=321)
tsne_representation = tsne.fit_transform(data_subset)

# In[32]:


colors = cm.rainbow(np.linspace(0, 1, len(set(response_subset))))
for y, c in zip(set(data.Response), colors):
    plt.scatter(tsne_representation[response_subset.values==y, 0], 
                tsne_representation[response_subset.values==y, 1], c=c, alpha=0.5, label=str(y))
_ = plt.legend()

# In[33]:


from sklearn.manifold import MDS
from sklearn.metrics.pairwise import pairwise_distances

# In[34]:


mds = MDS(random_state=321)
MDS_transformed = mds.fit_transform(data_subset)

# In[36]:


colors = cm.rainbow(np.linspace(0, 1, len(set(response_subset))))
for y, c in zip(set(response_subset), colors):
    plt.scatter(MDS_transformed[response_subset.values==y, 0], 
                MDS_transformed[response_subset.values==y, 1], 
                c=c, alpha=0.5, label=str(y))
plt.legend()
plt.xlim(-5, 5)   # масса точек концентриурется в этом масштабе
_ = plt.ylim(-5, 5)   # рекомендуем сначала отобразить визуализацию целиком, а замем раскомментировать эти строки.

# Для построения представления можно пробовать разные метрики, их список доступен в [документации scipy](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.pdist.html) (потому что sklearn использует метрики scipy).
# 
# В t_SNE метрику можно указать при создании объекта класса TSNE, в MDS это реализуется несколько сложнее. Нужно указать metric="precomputed", а в fit_transform подать не матрицу объектов, а матрицу попарных расстояний между объектами. Создать ее можно с помощью функции pairwise_distances с параметрами: матрица объектов, метрика.
# 
# Применим MDS с косинусной метрикой:

# In[39]:


mds = MDS(metric="precomputed", random_state=321)
MDS_transformed_cos = mds.fit_transform(pairwise_distances(data_subset, metric="cosine"))

# In[38]:


colors = cm.rainbow(np.linspace(0, 1, len(set(response_subset))))
for y, c in zip(set(response_subset), colors):
    plt.scatter(MDS_transformed_cos[response_subset.values[:subset_l]==y, 0], 
                MDS_transformed_cos[response_subset.values[:subset_l]==y, 1], 
                c=c, alpha=0.5, label=str(y))
_ = plt.legend()

# Мы видим, что точки с большим значением Response хорошо отделяются от точек в маленьким значением целевого признака, а вот промежуточные значения сильно перемешаны.

# ### Обзор решений участников соревнования
# Раз уж мы достаточно хорошо познакомились с данными соревнования, разберем основные приемы, которые применяли победители.
# 
# * Практически никакого feature engineering. Добавленные признаки: dummy для Product_Info_2, сумма признаков Medical_Keyword (редкие бинарные), кодирование бинарных признаков средними значениями Response по категории, произведение Ins_Age*BMI.
# * Способ работы с целочисленным целевым признаком (решение победителя): настройка семи XGBClassifier, i-й классификатор предсказывает вероятность того, что Response > i. Итоговое предсказание - сумма вероятностей, возвращенных семью классификаторами.
# * Итеративная процедура для настройки порогов с целью максимизации quadratic weighted kappa.
# * Схема локального контроля: кросс-валидация. Для настройки порога внутри кросс-валидации для каждого тестового фолда была сделана еще одна кросс-валидация с 3 фолдами.

# ## Примеры визуализаций

# ##### [Соревнование Сбербанка](https://contest.sdsj.ru/#/about?_k=mmogcn), восстановление исходных данных
# 
# Данные о пользовательских транзакциях по категориям, задача предсказания сумм трат.
# 
# Если в соревновании значения признаков даны участникам (признаки не анонимизированы), их часто стараются трансформировать так, чтобы объекты нельзя было распознать. Например, в недавно завершившемся соревновании Сбербанка вместо дат было дано число дней, прошедших с некоторого стартового дня. Кроме того, суммы транзакций были трансформированы. Однако с помощью простейшей визуализации можно было обратно восстановить исходные значения.
# 
# График общих сумм покупок по дням (из этой [визуализации А. Дьконова](https://github.com/Dyakonov/case_sdsj/blob/master/dj_sdsj01_visual.ipynb)):
# 
# ![](images/dj.png)
# 
# Точка резкого падения сумм - это Новый год. Значит, мы восстановили числа и месяцы.
# 
# Далее, если визуализировать суммы покупок мужчин в категории "цветы", то можно найти 14 февраля и 8 марта (из [визуализации В. Журавлева](https://kaggle2.blob.core.windows.net/forum-message-attachments/141450/5243/ZhuravlyovReport.pdf?sv=2015-12-11&sr=b&sig=zEIYp16pfUZ9ulRDjOBpP1Nr7Jv2c8tNP0lFfY3iScI%3D&se=2016-12-01T18%3A22%3A22Z&sp=r)):
# ![](images/vadim.png)
# 
# Тогда можно сделать вывод, что год невисокосный.

# ##### Соревнование [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction), поиск признаков
# 
# Задача многоклассовой классификации (с непересекающимися классами) - предсказать тип лесного покрова. 
# 
# Визуализация двух признаков из датасета с цветовой дифференциацией по классу:
# ![](https://habrastorage.org/files/cd4/fa1/55c/cd4fa155c3a6419eb5caac5c5524a217.png)
# 
# В результате визуализации добавляют разницу двух указанных признаков, и качество решения повышается.
# 
# [Статья на Хабрахабре](https://habrahabr.ru/company/mlclass/blog/249759/)

# ##### Соревнование [Bosch Production Line Performance](https://www.kaggle.com/c/bosch-production-line-performance/kernels), визуализация ID
# 
# Задача бинарной классификации с несбалансированными классами и большим числом разреженных признаков.
# 
# В ходе решения соревнования обнаружен "magic feature": если отсортировать объекты по времени и взять попарные (по соседним объектам) разности ID в качестве признака, то качество существенно повышается. 
# 
# Визуализация признака (по оси x - индекс объекта в датасете, по оси y - значение magic feature, левый и правый график соответствуют объектам со значением целевого признака 0 и 1 соответственно):
# 
# ![](https://www.kaggle.io/svf/389558/90eec3c5c3779ad32396cb7b6ba5c46f/__results___files/__results___11_0.png)
# 
# Для объектов класса 1 значения magic feature концентрируеются около 0.
# 
# [Ссылка на Kernel](https://www.kaggle.com/rithal/bosch-production-line-performance/magic-feature-visualization/comments)
