#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# 
# # Практическое задание 6
# 
# ## Общая информация
# 
# Дата выдачи: 03.04.2017
# 
# Срок сдачи: 16.04.2017 23:59MSK
# 
# ### О задании
# 
# Практическое задание 6 посвящено EM-алгоритму и его использовании в задачах классификации. В рамках данного задания вы:
#  * научитесь моделировать данные, сгенерированные из распределений сложной природы;
#  * реализуете ЕМ-алгоритм для 2 различных моделей генерации данных;
#  * изучите поведение оценок параметров распределений и скрытых переменных в процессе ЕМ-алгоритма;
#  * научитесь использовать ЕМ-алгоритм как метод обучения с учителем.
#  
#  
# ### Оценивание и штрафы
# 
# Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.
# 
# Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.
# 
# Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце Вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник). 
# 
# Неэффективная реализация кода может негативно отразиться на оценке.
# 
# 
# ### Формат сдачи
# Для сдачи задания переименуйте получившийся файл \*.ipynb в соответствии со следующим форматом: *HW6_Username.ipynb*, где *Username* — Ваша фамилия и инициалы на латинице (например, *HW6_IvanovII.ipynb*). Далее отправьте этот файл на hse.cs.ml+<номер группы>@gmail.com (например, hse.cs.ml+141@gmail.com для студентов группы БПМИ-141).

# # EM-алгоритм
# 
# ## Бинарная последовательность
# 
# Пусть наблюдается выборка бинарных значений $\mathbb{Y} = (y_1,\ldots, y_l), \; y_i\in\{0,1\}$. Все элементы выборки генерируются независимо, но известно, что в некоторый момент $z$ меняется частота генерации единиц, т.е.:
# $$\mathbb{P}(y_i = 1) = 
# \begin{cases}
# \theta_1, \, i < z,\\
# \theta_2, \, i \ge z. 
# \end{cases}$$
# 
# В рамках данной части задания вам необходимо будет реализовать ЕМ-алгоритм для данной модели, где $z$–скрытая переменная, а $\theta_1, \theta_2$ – параметры распределения.

# 1\. **(1 балл)** Выведите формулы Е- и М-шагов алгоритма для заданной модели. Напомним, что результатом Е-шага является распределение скрытых переменных $q(z)$, М-шага — оценки на параметры распределения.

# **E-шаг**.
# 
# \# Your code here
# 
# **М-шаг**.
# 
# \# Your code here

# 2\. **(2 балла)** Реализуйте ЕМ-алгоритм для заданной модели в виде функции, параметром которой является наблюдаемая выборка, а возвращаемым значением — векторы с оценками параметров распределений на каждой итерации алгоритма. В качестве критерия останова можете использовать ограничение количества итераций. Формат входных и выходных данных функции остаётся на ваше усмотрение.

# In[ ]:


# Your code here

# 3\. **(0.5 балла)** Проведите эксперимент для $\theta_1 = 0.1$, $\theta_2 = 0.9, z=50, l=100.$ Для этого выполните следующие действия:
# - сгенерируйте выборку для заданных истинных значений $\theta_1, \theta_2, z, l$;
# - примените реализованную в п. 2 функцию;
# - постройте графики зависимостей оценок $\theta_1, \theta_2, z$ от номера итерации ЕМ-алгоритма. 

# In[ ]:


# Your code here

# 4\. **(0.5 балла)** Какой функционал оптимизирует ЕМ-алгоритм? Постройте график зависимости этого функционала от номера итерации ЕМ-алгоритма. Ведёт ли эта величина себя монотонно?

# In[ ]:


# Your code here

# **Ответ:**

# 5\. **(0.5 балла)** Повторите эксперимент аналогично п.3 1000 раз для $\theta_1 = 0.49$, $\theta_2 = 0.51, z=500, l=550$. Оцените RMSE для между истинным значением и оценками, полученными в результате экспериментов, для $\theta_1, \theta_2, z$, а также постройте гистограмму полученных в результате экспериментов остатков для этих величин.

# In[ ]:


# Your code here

# ## Смесь нормальных распределений
# 
# Пусть данные описываются смесью многомерных нормальных распределений:
# $$p(x_i|\Theta) = \sum_{k=1}^K \pi_k p(x_i|\theta_k) = \sum_{k=1}^K \pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k).$$
# 
# В рамках данной части задания вам необходимо реализовать ЕМ-алгоритм для нахождения параметров указанного распределения (формулы для данной модели были выведены в материалах [семинара 15](https://github.com/esokolov/ml-course-hse/blob/master/2016-spring/seminars/sem15-EM.pdf)) и применить его для решения задачи классификации на датасете MNIST. Чтобы применение ЕМ-алгоритма к датасету MNIST было осмысленным, мы воспользуемся методом понижения размерности t-SNE.

# 7\. **(2 балла)** Реализуйте ЕМ-алгоритм для заданной модели в виде функции, параметром которой является наблюдаемая выборка, а возвращаемым значением — итоговые оценки параметров распределения и значения скрытых переменных. В качестве критерия останова можете использовать ограничение количества итераций. Формат входных и выходных данных функции остаётся на ваше усмотрение.

# In[ ]:


# Your code here

# 8\. **(1 балл)** Загрузите признаковые описания и значения целевой переменной для 2000 случайных объектов из обучающей выборки датасета [MNIST](http://yann.lecun.com/exdb/mnist/) и сохраните их в соответствующие переменные.

# In[ ]:


# Your code here

# 9\. **(0.5 балла)** Визуализируйте полученную выборку на плоскости при помощи t-SNE, используя различные цвета для объектов различных классов.

# In[ ]:


# Your code here

# 10\. **(0.5 балла)** Для выборки из п. 8 понизьте количество признаков до 3 при помощи t-SNE и примените функцию из п. 7 для разделения выборки на $K=10$ компонент.

# In[ ]:


# Your code here

# Полученное разделение выборки на компоненты можно использовать для построения классификатора, предсказывающего изображенную цифру. Для этого будем считать прогнозом для всех объектов $k$-ой компоненты самую частую истинную метку среди объектов этой компоненты:
# 
# $$a(x_i) = \arg \max_{k \in \{1, \dots, 10\}} \sum_{j=1}^l [z_j = z_i] [y_j = k],$$
# 
# где $z_i$ — вектор скрытых переменных для объекта выборки $x_i$.
# 
# 11\. **(0.5 балла)** Выведите значение accuracy на используемой выборке для описанного выше способа построения прогнозов на объектах и истинной разметки выборки.

# In[ ]:


# Your code here

# 12\. **(1 балл)** Постройте график зависимости значения accuracy на используемой выборке для описанного выше способа построения прогнозов на объектах и истинной разметки выборки от числа компонент $K$, фигурирующего в ЕМ-алгоритме.

# In[ ]:


# Your code here
