#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# 
# ## Практическое задание 3. Градиентный спуск своими руками
# 
# ### Общая информация
# Дата выдачи: 05.10.2019
# 
# Мягкий дедлайн: 07:59MSK 15.10.2019 (за каждый день просрочки снимается 1 балл)
# 
# Жесткий дедлайн: 23:59MSK 17.10.2019

# ### О задании
# 
# В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.
# 
# 
# ### Оценивание и штрафы
# Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.
# 
# Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.
# 
# Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).
# 
# Неэффективная реализация кода может негативно отразиться на оценке.
# 
# Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.
# 
# 
# ### Формат сдачи
# Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. 
# 
# Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.
# 
# **Оценка**: ...

# ## Реализация градиентного спуска
# 
# Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью:
# 
# ** Задание 1 (1 балл)** Градиентного спуска;
# 
# ** Задание 2 (1.5 балла)** Стохастического градиентного спуска;
# 
# ** Задание 3 (2.5 балла)** Метода Momentum.
# 
# 
# Во всех пунктах необходимо соблюдать следующие условия:
# 
# * Все вычисления должны быть векторизованы;
# * Циклы средствами python допускается использовать только для итераций градиентного спуска;
# * В качестве критерия останова необходимо использовать (одновременно):
# 
#     * проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);
#     * достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).
# * Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);
# * Инициализировать веса можно случайным образом или нулевым вектором. 
# 
# 
# Ниже приведён шаблон класса, который должен содержать код реализации каждого из методов.

# In[ ]:


import numpy as np
from sklearn.base import BaseEstimator

class LinearReg(BaseEstimator):
    def __init__(self, gd_type='stochastic', 
                 tolerance=1e-4, max_iter=1000, w0=None, alpha=1e-3, eta=1e-2):
        """
        gd_type: 'full' or 'stochastic' or 'momentum'
        tolerance: for stopping gradient descent
        max_iter: maximum number of steps in gradient descent
        w0: np.array of shape (d) - init weights
        eta: learning rate
        alpha: momentum coefficient
        """
        self.gd_type = gd_type
        self.tolerance = tolerance
        self.max_iter = max_iter
        self.w0 = w0
        self.alpha = alpha
        self.w = None
        self.eta = eta
        self.loss_history = None # list of loss function values at each training iteration
    
    def fit(self, X, y):
        """
        X: np.array of shape (ell, d)
        y: np.array of shape (ell)
        ---
        output: self
        """
        self.loss_history = []
        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
        return self
    
    def predict(self, X):
        if self.w is None:
            raise Exception('Not trained yet')
        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
        pass
    
    def calc_gradient(self, X, y):
        """
        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)
        y: np.array of shape (ell)
        ---
        output: np.array of shape (d)
        """
        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
        pass

    def calc_loss(self, X, y):
        """
        X: np.array of shape (ell, d)
        y: np.array of shape (ell)
        ---
        output: float 
        """ 
        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
        pass

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ** Задание 4 (0 баллов)**. 
# * Загрузите данные из домашнего задания 2 ([train.csv](https://www.kaggle.com/c/nyc-taxi-trip-duration/data));
# * Разбейте выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;
# * Преобразуйте целевую переменную `trip_duration` как $\hat{y} = \log{(y + 1)}$.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ** Задание 5 (3 балла)**. Обучите и провалидируйте модели на данных из предыдущего пункта, сравните качество между методами по метрикам MSE и $R^2$. Исследуйте влияние параметров `max_iter` и `eta` (`max_iter`, `alpha` и `eta` для Momentum) на процесс оптимизации. Согласуется ли оно с вашими ожиданиями?

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ** Задание 6 (2 балла)**. Постройте графики (на одной и той же картинке) зависимости величины функции потерь от номера итерации для полного, стохастического градиентного спусков, а также для полного градиентного спуска с методом Momentum. Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.
# 
# Не забывайте о том, что должны получиться *красивые* графики!

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ### Бонус 

# ** Задание 7 (2 балла)**. Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью метода
# [Adam](https://arxiv.org/pdf/1412.6980.pdf) - добавьте при необходимости параметры в класс модели, повторите пункты 5 и 6 и сравните результаты. 

# ** Задание 8 (2 балла)**. Реализуйте линейную регрессию с функцией потерь
# $$ L(\hat{y}, y) = log(cosh(\hat{y} - y)),$$
# 
# обучаемую с помощью градиентного спуска.

# ** Задание 9 (0.01 балла)**.  Вставьте картинку с вашим любимым мемом в этот Jupyter Notebook
