#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# 
# ## Практическое задание 2. Exploratory Data Analysis и линейная регрессия
# 
# ### Общая информация
# Дата выдачи: 25.09.2022
# 
# Мягкий дедлайн: 23:59MSK 10.10.2022
# 
# Жесткий дедлайн: 23:59MSK 18.10.2022

# ### О задании
# В этом задании мы попытаемся научиться анализировать данные и выделять из них полезные признаки. Мы также научимся пользоваться `seaborn` и `sklearn`, а заодно привыкнем к основным понятиям машинного обучения.
# 
# ### Оценивание и штрафы
# Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов. Проверяющий имеет право снизить оценку за неэффективную реализацию или неопрятные графики.
# 
# **Обратите внимание**, что в каждом разделе домашнего задания есть оцениваниемые задачи и есть вопросы. Вопросы дополняют задачи и направлены на то, чтобы проинтерпретировать или обосновать происходящее. Код без интерпретации не имеет смысла, поэтому отвечать на вопросы обязательно — за отсутствие ответов мы будем снижать баллы за задачи. Если вы ответите на вопросы, но не напишете корректный код к соответствующим оцениваемым задачам, то баллы за такое выставлены не будут.
# 
# Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.
# 
# Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).
# 
# ### Формат сдачи
# Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. Сам ноутбук называйте в формате homework-practice-02-linregr-Username.ipynb, где Username — ваша фамилия.
# 
# Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.
# 
# Оценка: xx.

# В этом ноутбуке используется библиотека `folium` для визуализации карт. Она работает в google colab!

# In[ ]:


!pip install folium

# In[ ]:


import folium

m = folium.Map(location=(55.7522200, 37.6155600), zoom_start=10)

m

# Если вы всё сделали правильно, то выше должна открыться карта Москвы.

# In[ ]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


sns.set(style="darkgrid")

# ## Часть 0. Подготовка (1 балл)

# **Задание 1 (1 балл)**. Мы будем работать с данными из соревнования [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration/overview), в котором нужно было предсказать длительность поездки на такси. Скачайте обучающую выборку из этого соревнования и загрузите ее:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Обратите внимание на колонки `pickup_datetime` и `dropoff_datetime`. `dropoff_datetime` был добавлена организаторами только в обучающую выборку, то есть использовать эту колонку нельзя, давайте удалим ее. В `pickup_datetime` записаны дата и время начала поездки. Чтобы с ней было удобно работать, давайте преобразуем даты в `datetime`-объекты

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# В колонке `trip_duration` записано целевое значение, которое мы хотим предсказывать. Давайте посмотрим на распределение таргета в обучающей выборке. Для этого нарисуйте его гистограмму:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Что можно сказать о целевой переменной по гистограмме её значений?

# В соревновании в качестве метрики качества использовалось RMSLE:
# $$\text{RMSLE}(X, y, a) = \sqrt{\frac{1}{\ell}\sum_{i=1}^{\ell} \big(\log{(y_i + 1)} - \log{(a(x_i) + 1)}\big)^2}$$
# 
# **Вопрос**: Как вы думаете, почему авторы соревнования выбрали именно RMSLE, а не RMSE?

# На семинаре мы рассматривали несколько моделей линейной регрессии в `sklearn`, но каждая из них оптимизировала среднеквадратичную ошибку (MSE), а не RMSLE. Давайте проделаем следующий трюк: будем предсказывать не целевую переменную, а ее *логарифм*. Обозначим $\hat{y}_i = \log{(y_i + 1)}$ — модифицированный таргет, а $\hat{a}(x_i)$ — предсказание модели, которая обучалась на $\hat{y}_i$, то есть логарифм таргета. Чтобы предсказать исходное значение, мы можем просто взять экспоненту от нашего предсказания: $a(x_i) = \exp(\hat{a}(x_i)) - 1$.
# 
# **Вопрос**: Покажите, что оптимизация RMSLE для модели $a$ эквивалентна оптимизации MSE для модели $\hat{a}$.
# 
# **Доказательство**: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
# 
# Итак, мы смогли свести задачу оптимизации RMSLE к задаче оптимизации MSE, которую мы умеем решать! Кроме того, у логарифмирования таргета есть еще одно полезное свойство. Чтобы его увидеть, добавьте к нашей выборке колонку `log_trip_duration` (воспользуйтесь `np.log1p`) и нарисуйте гистограмму модифицированного таргета по обучающей выборке. Удалите колонку со старым таргетом.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Чтобы иметь некоторую точку отсчета, давайте посчитаем значение метрики при наилучшем константном предсказании:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ## Часть 1. Изучаем `pickup_datetime` (2 балла)
# 
# **Задание 2 (0.25 баллов)**. Для начала давайте посмотрим, сколько всего было поездок в каждый из дней. Постройте график зависимости количества поездок от дня в году (например, можно воспользоваться `sns.countplot`):

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Вы, вероятно, заметили, что на графике есть 2 периода с аномально маленькими количествами поездок. Вычислите, в какие даты происходили эти скачки вниз и найдите информацию о том, что происходило в эти дни в Нью-Йорке.
# 
# Нарисуйте графики зависимости количества поездок от дня недели и от часов в сутках (воспользуйтесь `sns.relplot`):

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Задание 3 (0.5 баллов)**. Нарисуйте на одном графике зависимости количества поездок от часа в сутках для разных месяцев (разные кривые, соответствующие разным месяцам, окрашивайте в разные цвета, воспользуйтесь `hue` в `sns.relplot`). Аналогично нарисуйте зависимости количества поездок от часа в сутках для разных дней недели.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Какие выводы можно сделать, основываясь на графиках выше? Выделяются ли какие-нибудь дни недели? Месяца? Время суток? С чем это связано?
# 
# **Задание 4 (0.5 баллов)**. Разбейте выборку на обучающую и тестовую в отношении 7:3. По обучающей выборке нарисуйте график зависимости среднего логарифма времени поездки от дня недели. Затем сделайте то же самое, но для часа в сутках и дня в году.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Похожи ли графики зависимости таргета от дня недели и от часа в сутках на аналогичные графики для количества поездок? Почему? Что происходит со средним таргетом в те два аномальных периода, что мы видели выше? Почему так происходит? Наблюдаете ли вы какой-нибудь тренд на графике зависимости `log_trip_duration` от номера дня в году?

# Добавьте следующие признаки на основе `pickup_datetime`:
# 1. День недели
# 2. Месяц
# 3. Час
# 4. Является ли период аномальным (два бинарных признака, соответствующие двум аномальным периодам)
# 5. Номер дня в году

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Итак, мы уже создали некоторое количество признаков.
# 
# **Вопрос**: Какие из признаков стоит рассматривать как категориальные, а какие - как численные? Почему?

# **Задание 5 (0.75 баллов)**. Обучите `Ridge`-регрессию с параметрами по умолчанию, закодировав все категориальные признаки с помощью `OneHotEncoder`. Численные признаки отмасштабируйте с помощью `StandardScaler`. Используйте только признаки, которые мы выделили в этой части задания.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ## Часть 2. Изучаем координаты (3 балла)
# Мы уже очень хорошо изучили данные о времени начала поездки, давайте теперь посмотрим на информацию о координатах начала и конца поездки. Мы подготовили для вас функцию, которая на карте рисует точки начала или конца поездки. Примеры ее вызова вы найдете ниже. Обратите внимание, что в эту функцию мы передаем лишь небольшой кусочек данных, посколько иначе функция будет работать очень долго

# In[ ]:


def show_circles_on_map(data, latitude_column, longitude_column, color):
    """
    The function draws map with circles on it.
    The center of the map is the mean of coordinates passed in data.
    
    data: DataFrame that contains columns latitude_column and longitude_column
    latitude_column: string, the name of column for latitude coordinates
    longitude_column: string, the name of column for longitude coordinates
    color: string, the color of circles to be drawn
    """

    location = (data[latitude_column].mean(), data[longitude_column].mean())
    m = folium.Map(location=location)

    for _, row in data.iterrows():
        folium.Circle(
            radius=100,
            location=(row[latitude_column], row[longitude_column]),
            color=color,
            fill_color=color,
            fill=True
        ).add_to(m)

    return m

# In[ ]:


show_circles_on_map(df.sample(1000), "pickup_latitude", "pickup_longitude", "blue")

# In[ ]:


show_circles_on_map(df.sample(1000), "dropoff_latitude", "dropoff_longitude", "blue")

# **Вопрос**: Какие две точки выделяются на карте?

# **Задание 6 (0.75 балл)**. Как мы все прекрасно помним, $t = s / v_{\text{ср}}$, поэтому очевидно, что самым сильным признаком будет расстояние, которое необходимо проехать. Мы не можем посчитать точное расстояние, которое необходимо преодолеть такси, но мы можем его оценить, посчитав кратчайшее расстояние между точками начала и конца поездки. Чтобы корректно посчитать расстояние между двумя точками на Земле, можно использовать функцию `haversine`. Также можно воспользоваться кодом с первого семинара. Посчитайте кратчайшее расстояние для объектов и запишите его в колонку `haversine`:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Так как мы предсказываем логарифм времени поездки и хотим, чтобы наши признаки были линейно зависимы с этой целевой переменной, нам нужно логарифмировать расстояние: $\log t = \log s - \log{v_{\text{ср}}}$. Запишите логарифм `haversine` в отдельную колонку:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Убедимся, что логарифм расстояния лучше коррелирует с нашим таргетом, чем просто расстояние:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Задание 7 (0.75 балла)**. Давайте изучим среднюю скорость движения такси. Посчитайте среднюю скорость для каждого объекта обучающей выборки, разделив `haversine` на `trip_duration`, и нарисуйте гистограмму ее распределения

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Как можно видеть по гистограмме, для некоторых объектов у нас получились очень больше значения скоростей. Нарисуйте гистограмму по объектам, для которых значение скорости получилось разумным (например, можно не включать рассмотрение объекты, где скорость больше некоторой квантили):

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Для каждой пары (день недели, час суток) посчитайте медиану скоростей. Нарисуйте с помощью `sns.heatmap` график, где по осям будут дни недели и часы, а в качестве значения функции - медиана скорости

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Не забудьте удалить колонку со значением скорости из данных!
# 
# **Вопрос**: Почему значение скорости нельзя использовать во время обучения?

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Посмотрите внимательно на график и скажите, в какие моменты времени скорость минимальна; максимальна.
# 
# Создайте признаки "поездка совершается в период пробок" и "поездка совершается в период свободных дорог" (естественно, они не должен зависеть от скорости!):

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Задание 8 (0.25 балла)**. Как уже было замечено выше, на карте выделяются две точки вдали от Манхэттена. Для каждой из них добавьте в выборку два признака: началась ли поездка в ней и закончилась ли она в ней.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Для каждого из созданных признаков нарисуйте "ящик с усами" (`sns.boxplot`) распределения логарифма времени поездки

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: судя по графикам, как вы думаете, хорошими ли получились эти признаки?

# <img src="https://www.dropbox.com/s/xson9nukz5hba7c/map.png?raw=1" align="right" width="20%" style="margin-left: 20px; margin-bottom: 20px">
# 
# **Задание 9 (1 балл)**. Сейчас мы почти что не используем сами значения координат. На это есть несколько причин: по отдельности рассматривать широту и долготу не имеет особого смысла, стоит рассматривать их вместе. Во-вторых, понятно, что зависимость между нашим таргетом и координатами не линейная. Чтобы как-то использовать координаты, можно прибегнуть к следующему трюку: обрамим область с наибольшим количеством поездок прямоугольником (как на рисунке). Разобьем этот прямоугольник на ячейки. Каждой точке сопоставим номер ее ячейки, а тем точкам, что не попали ни в одну из ячеек, сопоставим значение -1.
# 
# Напишите трансформер, который сначала разбивает показанную на рисунке область на ячейки, а затем создает два признака: номер ячейки, в которой началась поездка, и номер ячейки, в которой закончилась поездка. Количество строк и столбцов выберите самостоятельно.
# 
# Обратите внимание, что все вычисления должны быть векторизованными, трансформер не должен модифицировать передаваемую ему выборку inplace, а все необходимые статистики (если они вдруг нужны) нужно считать только по обучающей выборке в методе `fit`:

# In[ ]:


from sklearn.base import BaseEstimator, TransformerMixin


# TransformerMixin implements fit_transform for you,
# applying your fit and transform consistently
    
class MapGridTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, ...):
        # your code here
        pass
    
    def show_map(self):
        # you may want to visualize cells
        pass
    
    def fit(self, X=None, y=None):
        # your code here
        return self

    def transform(self, X, y=None):
        # your code here
        return X

# **Задание 10 (0.25 балла)**. Обучите `Ridge`-регрессию со стандартными параметрами на признаках, которые мы выделили к текущему моменту. Категориальные признаки закодируйте через one-hot-кодирование, числовые признаки отмасштабируйте.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ## Часть 3. Изучаем оставшиеся признаки (1 балл)
# 
# **Задание 11 (0.75 баллов)**. У нас осталось еще 3 признака, которые мы не исследовали: `vendor_id`, `passenger_count` и `store_and_fwd_flag`.
# 
# **Вопрос**: Подумайте, почему каждый из этих признаков может быть потенциально полезным.
# 
# Посчитайте, сколько есть уникальных значений у каждого из этих признаков:

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Постройте "ящики с усами" распределений логарифма времени поездки в зависимости от значений каждого из признаков

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Переведите признаки `vendor_id` и `store_and_fwd_flag` в значения $\{0;1\}$

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Вопрос**: Основываясь на графиках выше, как вы думаете, будут ли эти признаки сильными?

# **Задание 12 (0.25 баллов)**. Проверьте свои предположения, обучив модель в том числе и на этих трех признаках. Обучайте `Ridge`-регрессию со стандартными параметрами. Категориальные признаки закодируйте one-hot-кодированием, а численные отмасштабируйте.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Если признаки не дали какого-то ощутимого улучшения метрики, их можно выбросить из данных.

# ## Часть 4. Улучшаем модель (3 балла)

# **Задание 13 (1 балл)**. В наших данных есть нетипичные объекты: с аномально маленьким времени поездки, с очень большим пройденным расстоянием или очень большими остатками регрессии. В этом задании предлагается исключить такие объекты из обучающей выборки. Для этого нарисуйте гистограммы распределения упомянутых выше величин, выберите объекты, которые можно назвать выбросами, и очистите обучающую выборку от них.
# 
# Отметим, что хотя эти объекты и выглядят как выбросы, в тестовой выборке тоже скорее всего будут объекты с такими же странными значениями целевой переменной и/или признаков. Поэтому, возможно, чистка обучающей выборки приведёт к ухудшению качества на тесте. Тем не менее, всё равно лучше удалять выбросы из обучения, чтобы модель получалась более разумной и интерпретируемой.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Сейчас у нас очень много категориальных признаков. В категориальных признаках могут содержаться редкие категории, обычно это плохо: модель сильно переобучается на таких примерах. Попробуйте объединить редкие категории в одну. Естественно, делать это нужно только для действительно редких категорий.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Обучите модель на очищенных данных и посчитайте качество на тестовой выборке.

# **Задание 14 (1 балл)**. После OneHot-кодирования количество признаков в нашем датасете сильно возрастает. Посчитайте колиество признаков до и после кодирования категориальных признаков.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Попробуйте обучить не `Ridge`-, а `Lasso`-регрессию. Какой метод лучше?

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Разбейте обучающую выборку на обучающую и валидационную в отношении 8:2. По валидационной выборке подберите оптимальные значения параметра регуляризации (по логарифмической сетке) для `Ridge` и `Lasso`, на тестовой выборке измерьте качество лучшей полученной модели.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Для каждого перебранного `alpha` для Lasso посчитайте количество нулевых весов в модели и нарисуйте график зависимости его от `alpha`. Как сильно придётся потерять в качестве, если мы хотим с помощью Lasso избавиться хотя бы от половины признаков?

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# <img src="https://www.dropbox.com/s/wp4jj0599np17lh/map_direction.png?raw=1" width="20%" align="right" style="margin-left: 20px">
# 
# **Задание 15 (1 балл)**. Часто бывает полезным использовать взаимодействия признаков (feature interactions), то есть строить новые признаки на основе уже существующих. Выше мы разбили карту Манхэттена на ячейки и придумали признаки "из какой ячейки началась поездка" и "в какой ячейке закончилась поездка".
# 
# Давайте попробуем сделать следующее: посчитаем, сколько раз встречается каждая возможная пара этих признаков в нашем датасете и выберем 100 самых частых пар. Закодируем поездки с этими частыми парами как категориальный признак, остальным объектам припишем -1. Получается, что мы закодировали, откуда и куда должно было ехать такси.
# 
# Также можете придумать ещё какой-нибудь способ сделать признаки про маршрут. Если эти признаки будут давать хороший прирост в качестве, то за это могут быть даны дополнительные бонусные баллы.
# 
# **Вопрос**: Почему такой признак потенциально полезный? Почему линейная модель не может самостоятельно "вытащить" эту информацию, ведь у нее в распоряжении есть признаки "из какой ячейки началась поездка" и "в какой ячейке закончилась поездка"?

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Заново обучите модель (`Ridge`, если она дала более высокое качество в предыдущих экспериментах, и `Lasso` иначе) на новых даннных и посчитайте качество на тестовой выборке

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Задание 16 (бонус, 1 балл)**. Где, как не для нашей задачи, считать манхэттенское расстояние?
# 
# **Вопрос**: Найдите, что такое манхэттенское расстояние и почему оно так называется. Как оно нам может помочь?
# 
# Введите систему координат на нашей карте так, чтобы оси были параллельны улицам Манхэттена, и добавьте сначала в данные признак "манхэттенское расстояние между пунктом отправления и пунктом назначения", а затем и логарифм этого признака. Посчитайте корреляцию между вашим новыми признаком и таргетом; между `log_haversine` и таргетом. В каком случае корреляция больше?
# 
# Нарисуйте карту, где покажете выбранные оси. Чтобы мы могли проверить вашу работу, просьба сделать скрин этой карты и приложить картинку (если мы откроем ваш ноутбук, виджеты отображаться не будут). 

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Заново обучите модель на новых даннных и посчитайте качество на тестовой выборке. Стало ли лучше? Объясните полученный результат.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# **Задание 17 (бонус, 2 балла)**. Реализуйте трансформер, который строит разбиение карты по шестигранной решётке с помощью библиотеки [H3](https://github.com/uber/h3-py) и вычисляет признаки на основе такого разбиения.
# 
# Признаки могут быть самые разные: расстояние между точкой старта и финиша, посчитанное в количестве шестиугольников; статистика по числу поездок и по их продолжительности в соседних шестиугольниках.
# 
# Важно: производительность библиотеки существенно зависит от количества шестиугольников на карте (определяется параметром resolution). Подберите такое разрешение, при котором ваш код будет работать за приемлемое время.
# 
# При построении признаков старайтесь не допустить утечки целевой переменной (подробнее про это можно почитать в материалах 1-го семинара) — в противном случае хорошего качества на тестовой выборке достичь не получится.
# 
# Измерьте качество после добавления новых признаков. За улучшение функционала ошибки на каждые 0.005 на тестовой выборке будет даваться 0.5 бонусных балла. Можно получить до 2 бонусных баллов за это задание.

# In[ ]:


#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# Вставьте картинку, описывающую ваш опыт выполнения этого ДЗ.

# In[ ]:



