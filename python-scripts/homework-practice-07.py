#!/usr/bin/env python
# coding: utf-8

# # Машинное обучение, ФКН ВШЭ
# 
# ## Практическое задание 7. Градиентный бустинг ~~своими руками~~
# 
# ### Общая информация
# Дата выдачи: 04.12.2019
# 
# Мягкий дедлайн: 05:59MSK 15.12.2019
# 
# Жесткий дедлайн: 05:59MSK 17.12.2019
# 
# ### Оценивание и штрафы
# Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 15.5 баллов.
# 
# Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.
# 
# Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).
# 
# Неэффективная реализация кода может негативно отразиться на оценке.
# 
# ### Формат сдачи
# Задания сдаются через систему anytask. Посылка должна содержать:
# * Ноутбук homework-practice-07-Username.ipynb
# 
# Username — ваша фамилия и имя на латинице именно в таком порядке

# __Задание 1. (0.5 балла)__
# 
# Мы будем использовать данные из [соревнования](https://www.kaggle.com/t/b710e05dc0bd424995ca94da5b639869). 
# * Загрузите таблицу application_train.csv;
# * Запишите в Y столбец с целевой переменной (TARGET);
# * Удалите ненужные столбцы (для этого воспользуйтесь описанием);
# * Определите тип столбцов и заполните пропуски - стратегия произвольная;
# * Разбейте выборку в соотношении 70:30 с random_state=0.
# 
# Так как в данных имеется значительный дисбаланс классов, в качестве метрики качества везде будем использовать площадь под precision-recall кривой (AUC-PR).

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 2. (1.5 балла)__
# 
# Обучите реализации градиентного бустинга LightGBM и Catboost на вещественных признаках без подбора параметров. 
# Почему получилась заметная разница в качестве? 
# 
# В этом и последующих экспериментах необходимо измерять время обучения моделей.

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 3. (2 балла)__
# 
# Подберите оптимальные с точки зрения метрики качества параметры алгоритмов, изменяя:
# 
# * глубину деревьев;
# * количество деревьев;
# * темп обучения.
# 
# Масштаб значений предлагается посмотреть в семинаре про библиотеки.
# 
# Проанализируйте соотношения глубины и количества деревьев в зависимости от алгоритма. 

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 4. (3.5 балла)__
# 
# Добавьте категориальные признаки к вещественным следующими способами:
# 
# * как OHE признаки;
# * как счетчики со сглаживанием.
# 
# При подсчете счетчиков запрещается использование циклов. 
# 
# Как меняется время, необходимое для обучения моделей в зависимости от способа кодирования? Сравните полученные результаты с встроенными методами обработки категориальных признаков. 

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 5. (1 балл)__
# 
# Реализуйте блендинг подобранных в предыдущем задании моделей и сравните качество. Обратите внимание на данные, на которых обучаются 

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 6. (1.5 балла)__
# 
# В задании 3 вы подобрали гиперпараметры для LightGBM и CatBoost на вещественных признаках. Визуализируйте важности признаков, посчитанные этими алгоритмами, в виде горизонтального bar-plot (отсортируйте признаки по убыванию важности, подпишите названия признаков по оси y).
# 
# Для каждого из двух алгоритмов удалите неважные признаки (обычно по bar-plot хорошо видно порог, с которого начинается "хвост" менее важных признаков) и обучите модель с теми же параметрами на получившихся данных. Сильно ли упало качество при удалении признаков?

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# ### Бонус
# 
# __Задание 7. (Максимум 5 баллов)__
# 
# Градиентный бустинг при всех ограничениях зачастую является ультимативным решением для задач с табличными данными; соревнование от Home Credit не стало исключением. Предлагается любыми модификациями Catboost и LightGBM (другими моделями пользоваться нельзя), обработки данных и построения признаков получить высокое качество на __приватном__ наборе данных. 
# Баллы можно получить за следующие значения метрики в inclass-соревновании на Kaggle:
# * 0.7955 - 1 балл
# * 0.7975 - 3 баллов
# * 0.80 - 5 баллов.
# 
# При сдаче этого задания необходимо приложить код, с помощью которого можно получить ту же самую метрику, что и в вашем посылке в соревновании. Если решение не будет воспроизводимым, задание засчитано не будет.
# 
# Учетная запись на kaggle: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

# __Задание 8. (0.5 балла)__ Обратите внимание, что в соревновании метрикой выступает AUC-ROC, у нас же до этого оптимизировался AUC-PR. Можно ли утверждать, что нет необходимости в переобучении моделей для максимизации новой метрики, если мы нашли лучший алгоритм с точки зрения AUC-PR?

# In[ ]:


### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
